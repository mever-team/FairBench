# Assessing LLMs

You can use FairBench to assess the fairness of LLM models under synthetic prompts
to uncover explicit or implicit biases.

!!! warning 
    The prompts and prompt templates described in this documentation and implemented
    in the library may reflect biases - and are deliberately engineered to attempt to 
    induce more biased answers than normal. This is done, so that discrepancies
    between groups, or between biased and unbiased behavior, 
    can be uncovered by qualitative and quantitative assessment. 
    To promote responsible usage, this warning will be shown by the library
    when calling the interfaces described below.

!!! warning
    DO NOT BLINDLY USE THESE OUTCOMES FOR TRAINING NEW SYSTEMS OR AS INDICATIVE
    OF THE TOTAL BELIEFS ENCODED IN INVESTIGATED MODELS.

**This section is under construction.**
